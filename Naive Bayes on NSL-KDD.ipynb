{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22545, 41)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('NSL-KDD-Dataset/KDDTrain+.csv')\n",
    "test_data = pd.read_csv('NSL-KDD-Dataset/KDDTest+.csv')\n",
    "#train_data[train_data['service']!='-']\n",
    "cols_to_transform = ['protocol_type', 'services', 'flag']\n",
    "#train_data = pd.get_dummies(data = train_data, columns = cols_to_transform)\n",
    "#test_data = pd.get_dummies(data = test_data, columns = cols_to_transform)\n",
    "#train_data[['label','attack_cat']][train_data['attack_cat']!='Dos'].head(10000)\n",
    "#obj_df = train_data.select_dtypes(include=['object']).copy()\n",
    "for col in cols_to_transform:\n",
    "    train_data[col]= train_data[col].astype('category').cat.codes\n",
    "    test_data[col]= test_data[col].astype('category').cat.codes\n",
    "train_data_ = train_data.drop(['num_outbound_cmds','class_label'], axis=1)\n",
    "test_data_ = test_data.drop(['num_outbound_cmds','class_label'], axis=1)\n",
    "train_data_ = train_data_.as_matrix(columns = None)\n",
    "test_data_ = test_data_.as_matrix(columns = None)\n",
    "test_data_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= train_data_[:,:-1]\n",
    "y_train = train_data_[:,-1]\n",
    "x_test = test_data_[:,:-1]\n",
    "y_test = test_data_[:,-1]\n",
    "x_min = []\n",
    "x_max = []\n",
    "for i in range(x_train.shape[1]):\n",
    "    x_min.append(np.min(x_train[:,i]))\n",
    "    x_max.append(np.max(x_train[:,i]))\n",
    "x_max = np.array(x_max)\n",
    "x_min = np.array(x_min)\n",
    "x_train = (x_train - x_min)/(x_max-x_min)\n",
    "x_test = (x_test - x_min)/(x_max-x_min)\n",
    "x_test = x_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "dim_size = 18\n",
    "epochs = 20\n",
    "num_seq = 100\n",
    "# Batch Size\n",
    "batch_size =200\n",
    "# RNN Size\n",
    "rnn_size =50\n",
    "# Learning Rate\n",
    "learning_rate = 0.00008\n",
    "num_layers = 1\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "keep_prob = 1\n",
    "relu_size = 50\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32236566e+00, -2.28826003e-01,  9.68252263e-02,\n",
       "         4.86907317e-01, -3.37092847e-01,  3.10851823e-01,\n",
       "        -2.20936845e-01,  1.84154175e-01, -3.05602369e-01,\n",
       "         1.16690320e-01,  8.43966141e-02, -1.07548648e-01,\n",
       "        -1.61493224e-01,  4.74326274e-02,  1.16950327e-01,\n",
       "        -1.34933275e-01, -2.40885055e-02, -9.48888489e-02],\n",
       "       [ 1.35046069e+00,  2.59793799e-01,  4.35092859e-01,\n",
       "         7.80733812e-01, -6.45491350e-01,  7.52956059e-01,\n",
       "        -9.14937032e-02,  1.68794485e-02,  4.23706159e-01,\n",
       "        -3.87287405e-01, -1.98567432e-01,  4.07497663e-01,\n",
       "         2.55590568e-01,  3.77393376e-02, -3.81232140e-02,\n",
       "         9.42956877e-03,  2.12495512e-01, -1.80928064e-02],\n",
       "       [ 1.60048583e+00,  1.84573252e+00, -2.73237501e-01,\n",
       "        -5.69528671e-02,  1.09015601e-01, -1.97459521e-02,\n",
       "        -1.44628636e-03,  5.26880667e-02,  5.31515125e-02,\n",
       "         6.27603752e-03, -1.58721403e-01,  1.62501842e-03,\n",
       "        -9.93925572e-03,  1.05593398e-02,  9.74464693e-03,\n",
       "         1.70538228e-02,  9.72845249e-03, -1.24689493e-02],\n",
       "       [ 1.99128661e+00, -8.65504134e-01, -4.29726630e-01,\n",
       "        -3.20792351e-01,  3.69807167e-01, -1.16446727e-01,\n",
       "        -2.52987357e-01, -8.35604122e-02,  1.20947763e-01,\n",
       "         4.94977912e-03,  6.34073640e-02, -2.70756255e-02,\n",
       "         2.48546081e-02,  5.86737359e-03,  8.05018706e-02,\n",
       "        -2.91292634e-02, -1.54997483e-02,  7.75373577e-03],\n",
       "       [ 2.28492099e+00, -8.08617446e-01, -1.91503762e-01,\n",
       "        -3.68664508e-01, -2.63553304e-01, -5.66530421e-02,\n",
       "         1.86094105e-01, -1.08643090e-01, -8.41856435e-02,\n",
       "        -7.93081748e-02, -9.32402438e-02, -3.67085872e-02,\n",
       "         1.17239596e-02,  3.36554835e-02, -6.71553819e-02,\n",
       "         2.15951771e-02, -1.49041250e-03,  1.42072305e-02],\n",
       "       [ 1.07892041e+00,  3.64247101e-01,  2.07708749e+00,\n",
       "        -3.52728372e-01,  3.57631738e-02, -5.40940314e-02,\n",
       "         8.51085364e-02,  8.52702580e-02, -9.44489762e-02,\n",
       "         1.55932385e-01, -1.95298343e-01,  1.71332953e-01,\n",
       "        -4.76697979e-02,  3.77259463e-02,  4.69801535e-02,\n",
       "        -1.23781507e-02, -9.92734530e-03, -3.33872565e-02],\n",
       "       [ 1.56818840e+00,  1.88905577e+00, -2.53950516e-01,\n",
       "        -4.05890282e-02,  9.05041773e-02,  2.39678036e-02,\n",
       "         1.06480813e-02,  3.10842279e-02,  4.68415227e-02,\n",
       "         6.44417574e-02, -1.02294776e-01,  2.21547279e-02,\n",
       "        -3.69318550e-03,  1.88388057e-03,  1.13196884e-03,\n",
       "         1.03241327e-02,  1.61401460e-02, -2.30179589e-03],\n",
       "       [ 1.61091643e+00,  1.84041492e+00, -2.64396687e-01,\n",
       "        -2.32499239e-02,  1.10987834e-01,  2.80422307e-02,\n",
       "        -2.85433478e-02,  5.96896732e-02, -7.56403367e-03,\n",
       "         3.06143853e-02, -1.29275316e-01, -4.32452053e-02,\n",
       "        -2.79555477e-03, -3.43845474e-03,  4.04833272e-04,\n",
       "         1.50949905e-02, -6.08864874e-03, -1.63198592e-02],\n",
       "       [ 1.64093172e+00,  1.87978243e+00, -2.44295596e-01,\n",
       "        -3.09397187e-02,  7.53343726e-02, -4.90441874e-02,\n",
       "         3.97194045e-02, -2.98741552e-02,  1.16615410e-01,\n",
       "         1.85988896e-01, -3.62750664e-02,  2.42735880e-02,\n",
       "         3.23266034e-02, -1.14146283e-02, -1.97894990e-02,\n",
       "         2.55762206e-02,  3.14716527e-02,  2.30965991e-02],\n",
       "       [ 1.57507925e+00,  1.87160802e+00, -2.60834812e-01,\n",
       "        -4.28184763e-02,  1.00779537e-01,  2.54320242e-02,\n",
       "        -4.06104952e-03,  4.98661330e-02,  3.11115581e-02,\n",
       "         2.95614800e-02, -1.31178014e-01,  2.92767066e-03,\n",
       "        -3.29580803e-03,  6.40519992e-03,  5.21282720e-03,\n",
       "         1.17127603e-02,  1.12289388e-02, -1.06908235e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = dim_size)\n",
    "x_reduced = svd.fit_transform(x_train)\n",
    "x_reduced[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_reduced = svd.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(arr,y, num_seq):\n",
    "    x_resampled ,y_resampled = SMOTE().fit_sample(arr, y)\n",
    "    return x_resampled,y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resampled = []\n",
    "y_resampled = []\n",
    "for i in range(y_train.size//25000):\n",
    "    x_resampledi, y_resampledi = resample(x_reduced[i*25000:25000*(i+1)], y_train[i*25000:25000*(i+1)],num_seq)\n",
    "    x_resampled.append(x_resampledi)\n",
    "    y_resampled.append(y_resampledi)\n",
    "#x_resampledj,y_resampledj = resample(x_reduced[75000:], y_train[75000:],num_seq)\n",
    "#x_resampled.append(x_resampledj)\n",
    "#y_resampled.append(y_resampledj)\n",
    "x_resampled = np.array(np.concatenate(x_resampled, axis = 0))    \n",
    "y_resampled = np.array(np.concatenate(y_resampled, axis = 0))\n",
    "from random import shuffle\n",
    "ind_list = [i for i in range(x_resampled.shape[0])]\n",
    "shuffle(ind_list)\n",
    "x_train_ = x_resampled[ind_list]\n",
    "y_train_ = y_resampled[ind_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.499125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid = x_train_[-40000:]\n",
    "y_valid = y_train_[-40000:]\n",
    "train_x = x_train_[:-40000]\n",
    "train_y = y_train_[:-40000]\n",
    "x_train.shape\n",
    "np.mean(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.28985507, ..., 0.        , 0.05      ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.63768116, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.5       , 0.71014493, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.5       , 0.7826087 , ..., 0.        , 0.01      ,\n",
       "        0.        ],\n",
       "       [0.        , 0.5       , 0.43478261, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.5       , 0.28985507, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_x,train_y)\n",
    "val_pred = clf.predict(x_valid)\n",
    "test_pred = clf.predict(x_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118862, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18141, 89, 1824, 18141)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn,fp,fn,tp = confusion_matrix(y_valid,val_pred).ravel()\n",
    "(tp,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9951179374657159,\n",
       " 0.004442226104317444,\n",
       " 0.9086401202103681,\n",
       " 0.949914910328577,\n",
       " 0.952175)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4596, 146, 1145, 16657)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_test[:-1],test_pred).ravel()\n",
    "(tp,fp,fn,tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9692113032475749,\n",
       " 0.008688924596798191,\n",
       " 0.8005573941821982,\n",
       " 0.8768482304683773,\n",
       " 0.9427342086586231)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9963832299970475,\n",
       " 0.0032998855141760388,\n",
       " 0.9080146638415229,\n",
       " 0.9501486916891024,\n",
       " 0.9523312749238613)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = clf.predict(train_x)\n",
    "tn,fp,fn,tp = confusion_matrix(train_y,train_pred).ravel()\n",
    "\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-016c89b579c3>:76: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "epoch 0: Training loss : 0.551019\n",
      "Validation loss : 0.332696\n",
      "epoch 1: Training loss : 0.225657\n",
      "Validation loss : 0.181597\n",
      "epoch 2: Training loss : 0.174013\n",
      "Validation loss : 0.171841\n",
      "epoch 3: Training loss : 0.169996\n",
      "Validation loss : 0.170896\n",
      "epoch 4: Training loss : 0.169372\n",
      "Validation loss : 0.170925\n",
      "epoch 5: Training loss : 0.169221\n",
      "Validation loss : 0.170944\n",
      "epoch 6: Training loss : 0.169164\n",
      "Validation loss : 0.170983\n",
      "epoch 7: Training loss : 0.169136\n",
      "Validation loss : 0.171004\n",
      "epoch 8: Training loss : 0.169121\n",
      "Validation loss : 0.171015\n",
      "epoch 9: Training loss : 0.169111\n",
      "Validation loss : 0.171028\n",
      "epoch 10: Training loss : 0.169096\n",
      "Validation loss : 0.171039\n",
      "epoch 11: Training loss : 0.169086\n",
      "Validation loss : 0.171048\n",
      "epoch 12: Training loss : 0.169082\n",
      "Validation loss : 0.171056\n",
      "epoch 13: Training loss : 0.169075\n",
      "Validation loss : 0.171063\n",
      "epoch 14: Training loss : 0.169069\n",
      "Validation loss : 0.171072\n",
      "epoch 15: Training loss : 0.169057\n",
      "Validation loss : 0.171093\n",
      "epoch 16: Training loss : 0.169047\n",
      "Validation loss : 0.171098\n",
      "epoch 17: Training loss : 0.169027\n",
      "Validation loss : 0.171125\n",
      "epoch 18: Training loss : 0.169021\n",
      "Validation loss : 0.171125\n",
      "epoch 19: Training loss : 0.169008\n",
      "Validation loss : 0.171102\n",
      "Training accuracy : 95.23\n",
      "validation accuracy : 95.23\n",
      "Test accuracy : 94.28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9692340335242945,\n",
       " 0.00868471490177288,\n",
       " 0.8008415147265077,\n",
       " 0.8770279351060766,\n",
       " 0.9428125)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batches(arr,y, batch_size, num_seq=num_seq):\n",
    "    new_arr = np.zeros((arr.size,num_seq))\n",
    "    new_y = np.zeros(arr.size)\n",
    "    for i in range(arr.size-num_seq):\n",
    "        for j in range(num_seq):\n",
    "            new_arr[i,j] = arr[i+j]\n",
    "        new_y[i] = y[i+num_seq-1]\n",
    "    n_batches = arr.shape[0]//batch_size\n",
    "    for i in range(0,n_batches*batch_size,batch_size):\n",
    "        x = new_arr[i:i+batch_size,:]\n",
    "        y = new_y[i:i+batch_size]\n",
    "        yield x,y\n",
    "x, y = next(get_batches(train_pred, train_y,200))       \n",
    "x.shape\n",
    "\n",
    "def build_inputs(batch_size, num_inputs = 100 ):\n",
    "    inputs = tf.placeholder(tf.float32, [None,num_inputs], name = 'inputs')\n",
    "    outputs = tf.placeholder(tf.int32, [None,],name = 'targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, outputs, keep_prob\n",
    "build_inputs(200)\n",
    "\n",
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, initial_state   \n",
    "\n",
    "def build_output(lstm_output, in_size, out_size, relu_size = 100):\n",
    "    out = tf.contrib.layers.fully_connected(lstm_output, relu_size, activation_fn = tf.nn.relu)\n",
    "    out = tf.layers.batch_normalization(out)\n",
    "    logits = tf.contrib.layers.fully_connected(out,out_size, activation_fn=None)\n",
    "    #logits = tf.reshape(logits, (-1,10,3, out_size))\n",
    "    return logits\n",
    "\n",
    "\n",
    "def build_loss(logits, targets,vocab_size):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "      # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    #logits = tf.reshape(logits,[-1,vocab_size])\n",
    "    y_one_hot = tf.one_hot(targets, vocab_size)\n",
    "    #y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    #y_reshaped = tf.reduce_sum(y_one_hot, axis = 1)\n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss, y_one_hot\n",
    "\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "class RNN:\n",
    "    \n",
    "    def __init__(self, vocab_size, relu_size, batch_size=64,\n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_inputs = num_seq)\n",
    "        \n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "       \n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, tf.reshape(self.inputs,[-1,num_seq,1]), initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.logits = build_output(outputs, lstm_size, vocab_size, relu_size = relu_size)[:,-1]\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss, self.y_reshaped = build_loss(self.logits, self.targets, vocab_size)\n",
    "        self.pred = tf.argmax(self.logits,1)\n",
    "        #_,self.pred = tf.nn.top_k(self.logits,3)\n",
    "        correct_pred = tf.equal(self.pred, tf.argmax(self.y_reshaped,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "\n",
    "save_every_n = 200\n",
    "\n",
    "model = RNN(2,relu_size, batch_size=batch_size,\n",
    "                lstm_size=rnn_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        train_loss = []\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(train_pred, train_y,batch_size):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            train_loss.append(batch_loss)\n",
    "            #if(counter%show_every_n_batches ==0):\n",
    "                #print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  #'Training Step: {}... '.format(counter),\n",
    "                    #'Training loss: {:.4f}... '.format(batch_loss))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, num_layers))\n",
    "        val_loss = []\n",
    "        val_state = sess.run(model.initial_state)\n",
    "        for x, y in get_batches(val_pred, y_valid, batch_size):\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "            batch_loss, val_state = sess.run([model.loss, \n",
    "                                                 model.final_state], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "                                                 \n",
    "            val_loss.append(batch_loss)\n",
    "            \n",
    "        print(\"epoch {}: Training loss : {:.6f}\".format(e, np.mean(train_loss)))\n",
    "        print(\"Validation loss : {:.6f}\".format(np.mean(val_loss)))\n",
    "        \n",
    "    val_acc = []\n",
    "    valid_pred =[]\n",
    "    val_y = []\n",
    "    val_logits = []\n",
    "    val_state = sess.run(model.initial_state)\n",
    "    for x, y in get_batches(val_pred, y_valid,batch_size):\n",
    "        feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "        val_state,batch_pred,batch_acc,batch_logits = sess.run([model.final_state,model.pred,model.accuracy, model.logits], feed_dict = feed)\n",
    "        val_acc.append(batch_acc)\n",
    "        val_y.append(y)\n",
    "        valid_pred.append(batch_pred)\n",
    "        val_logits.append(batch_logits)\n",
    "    training_pred =[]\n",
    "    train_label = []\n",
    "    train_acc = []\n",
    "    train_logits = []\n",
    "    train_state = sess.run(model.initial_state)\n",
    "    for x, y in get_batches(train_pred, train_y,batch_size):\n",
    "        feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: train_state}\n",
    "        train_state,batch_pred,batch_acc, batch_logits = sess.run([model.final_state,model.pred,model.accuracy, model.logits], feed_dict = feed)\n",
    "        train_acc.append(batch_acc)\n",
    "        train_label.append(y)\n",
    "        training_pred.append(batch_pred)\n",
    "        train_logits.append(batch_logits)\n",
    "    print(\"Training accuracy : {:.2f}\".format(100*np.mean(train_acc)))    \n",
    "    print(\"validation accuracy : {:.2f}\".format(100*np.mean(val_acc)))\n",
    "    test_acc = []\n",
    "    test_pred_ =[]\n",
    "    test_y = []\n",
    "    test_logits = []\n",
    "    test_state = sess.run(model.initial_state)\n",
    "    for x, y in get_batches(test_pred, y_test,batch_size,num_seq):\n",
    "        feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "        test_state,batch_pred,batch_acc, batch_logits = sess.run([model.final_state,model.pred,model.accuracy,model.logits], feed_dict = feed)\n",
    "        test_acc.append(batch_acc)\n",
    "        test_y.append(y)\n",
    "        test_pred_.append(batch_pred)\n",
    "        test_logits.append(batch_logits)\n",
    "    print(\"Test accuracy : {:.2f}\".format(100*np.mean(test_acc))) \n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(np.concatenate(test_y),np.concatenate(test_pred_)).ravel()\n",
    "(tp,fp,fn,tn)\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9951050489495105,\n",
       " 0.0044309469282087025,\n",
       " 0.908556794215125,\n",
       " 0.9498635027299455,\n",
       " 0.95225)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tn,fp,fn,tp = confusion_matrix(np.concatenate(val_y),np.concatenate(valid_pred)).ravel()\n",
    "(tp,fp,fn,tn)\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9963802910541479,\n",
       " 0.003300663500724125,\n",
       " 0.9080076744420883,\n",
       " 0.9501435288730783,\n",
       " 0.9523400673400674)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(np.concatenate(train_label),np.concatenate(training_pred)).ravel()\n",
    "(tp,fp,fn,tn)\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
