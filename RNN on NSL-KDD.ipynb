{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22545, 41)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('NSL-KDD-Dataset/KDDTrain+.csv')\n",
    "test_data = pd.read_csv('NSL-KDD-Dataset/KDDTest+.csv')\n",
    "#train_data[train_data['service']!='-']\n",
    "cols_to_transform = ['protocol_type', 'services', 'flag']\n",
    "#train_data = pd.get_dummies(data = train_data, columns = cols_to_transform)\n",
    "#test_data = pd.get_dummies(data = test_data, columns = cols_to_transform)\n",
    "#train_data[['label','attack_cat']][train_data['attack_cat']!='Dos'].head(10000)\n",
    "#obj_df = train_data.select_dtypes(include=['object']).copy()\n",
    "for col in cols_to_transform:\n",
    "    train_data[col]= train_data[col].astype('category').cat.codes\n",
    "    test_data[col]= test_data[col].astype('category').cat.codes\n",
    "train_data_ = train_data.drop(['num_outbound_cmds','class_label'], axis=1)\n",
    "test_data_ = test_data.drop(['num_outbound_cmds','class_label'], axis=1)\n",
    "train_data_ = train_data_.as_matrix(columns = None)\n",
    "test_data_ = test_data_.as_matrix(columns = None)\n",
    "test_data_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= train_data_[:,:-1]\n",
    "y_train = train_data_[:,-1]\n",
    "x_test = test_data_[:,:-1]\n",
    "y_test = test_data_[:,-1]\n",
    "x_min = []\n",
    "x_max = []\n",
    "for i in range(x_train.shape[1]):\n",
    "    x_min.append(np.min(x_train[:,i]))\n",
    "    x_max.append(np.max(x_train[:,i]))\n",
    "x_max = np.array(x_max)\n",
    "x_min = np.array(x_min)\n",
    "x_train = (x_train - x_min)/(x_max-x_min)\n",
    "x_test = (x_test - x_min)/(x_max-x_min)\n",
    "x_test = x_test[:-1]\n",
    "y_test = y_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "dim_size = 30\n",
    "epochs = 200\n",
    "num_seq = 1\n",
    "# Batch Size\n",
    "batch_size =200\n",
    "# RNN Size\n",
    "rnn_size =50\n",
    "# Learning Rate\n",
    "learning_rate = 0.0005\n",
    "num_layers = 1\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "keep_prob = 0.5\n",
    "relu_size = 50\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32236566e+00, -2.28826003e-01,  9.68252263e-02,\n",
       "         4.86907317e-01, -3.37092847e-01,  3.10851823e-01,\n",
       "        -2.20936845e-01,  1.84154175e-01, -3.05602369e-01,\n",
       "         1.16690320e-01,  8.43966141e-02, -1.07548648e-01,\n",
       "        -1.61493224e-01,  4.74326274e-02,  1.16950327e-01,\n",
       "        -1.34933274e-01, -2.40885055e-02, -9.48888487e-02,\n",
       "        -3.53538221e-02, -9.47714734e-02,  7.66154260e-02,\n",
       "        -9.05532430e-02,  2.28095738e-03, -1.34439430e-02,\n",
       "         1.56069131e-03, -2.06610855e-02, -3.22240112e-04,\n",
       "        -7.58615677e-04,  2.53775065e-03,  3.40515422e-04],\n",
       "       [ 1.35046069e+00,  2.59793799e-01,  4.35092859e-01,\n",
       "         7.80733812e-01, -6.45491350e-01,  7.52956059e-01,\n",
       "        -9.14937032e-02,  1.68794485e-02,  4.23706159e-01,\n",
       "        -3.87287405e-01, -1.98567432e-01,  4.07497663e-01,\n",
       "         2.55590568e-01,  3.77393376e-02, -3.81232140e-02,\n",
       "         9.42956878e-03,  2.12495512e-01, -1.80928062e-02,\n",
       "        -1.43276516e-02, -7.25561463e-02, -2.22065042e-02,\n",
       "         1.47108525e-01, -1.44205004e-01,  1.12404376e-03,\n",
       "         1.29708631e-02,  2.83773303e-02,  9.06007517e-03,\n",
       "         3.01702135e-03,  1.71798497e-03,  6.34588518e-04],\n",
       "       [ 1.60048583e+00,  1.84573252e+00, -2.73237501e-01,\n",
       "        -5.69528671e-02,  1.09015601e-01, -1.97459521e-02,\n",
       "        -1.44628636e-03,  5.26880667e-02,  5.31515125e-02,\n",
       "         6.27603752e-03, -1.58721403e-01,  1.62501842e-03,\n",
       "        -9.93925572e-03,  1.05593398e-02,  9.74464693e-03,\n",
       "         1.70538228e-02,  9.72845248e-03, -1.24689494e-02,\n",
       "        -9.85350797e-04,  9.40409260e-03, -2.18792947e-02,\n",
       "         4.02285783e-02, -1.79922629e-02,  4.69223400e-03,\n",
       "         3.10833206e-05, -1.20592980e-04,  1.38780123e-03,\n",
       "        -5.35281990e-05, -2.58193882e-04, -2.09323205e-04],\n",
       "       [ 1.99128661e+00, -8.65504134e-01, -4.29726630e-01,\n",
       "        -3.20792351e-01,  3.69807167e-01, -1.16446727e-01,\n",
       "        -2.52987357e-01, -8.35604122e-02,  1.20947763e-01,\n",
       "         4.94977912e-03,  6.34073640e-02, -2.70756255e-02,\n",
       "         2.48546081e-02,  5.86737359e-03,  8.05018706e-02,\n",
       "        -2.91292634e-02, -1.54997483e-02,  7.75373573e-03,\n",
       "         8.81499684e-02, -1.23395596e-01,  2.26508558e-02,\n",
       "         7.80384690e-04, -2.94442589e-02,  5.21246877e-02,\n",
       "        -2.66577059e-03,  2.19333530e-02,  1.19969973e-02,\n",
       "         7.66087693e-04, -1.04085613e-03,  6.14851814e-04],\n",
       "       [ 2.28492099e+00, -8.08617446e-01, -1.91503762e-01,\n",
       "        -3.68664508e-01, -2.63553304e-01, -5.66530421e-02,\n",
       "         1.86094105e-01, -1.08643090e-01, -8.41856435e-02,\n",
       "        -7.93081748e-02, -9.32402438e-02, -3.67085872e-02,\n",
       "         1.17239596e-02,  3.36554835e-02, -6.71553819e-02,\n",
       "         2.15951771e-02, -1.49041252e-03,  1.42072305e-02,\n",
       "         5.82209245e-03,  6.62727941e-03, -1.64217009e-03,\n",
       "         3.12635366e-03,  5.04860744e-03, -5.65138012e-03,\n",
       "        -2.14123537e-03,  2.05582524e-03, -2.42444048e-03,\n",
       "         2.22943847e-04,  1.78025698e-04, -1.14216899e-03],\n",
       "       [ 1.07892041e+00,  3.64247101e-01,  2.07708749e+00,\n",
       "        -3.52728372e-01,  3.57631738e-02, -5.40940314e-02,\n",
       "         8.51085364e-02,  8.52702580e-02, -9.44489762e-02,\n",
       "         1.55932385e-01, -1.95298343e-01,  1.71332953e-01,\n",
       "        -4.76697979e-02,  3.77259463e-02,  4.69801535e-02,\n",
       "        -1.23781507e-02, -9.92734528e-03, -3.33872564e-02,\n",
       "         3.27349186e-02,  1.33302688e-02, -1.89715490e-02,\n",
       "         2.87298444e-02, -4.08094813e-02, -5.00356550e-03,\n",
       "         1.69280419e-03,  1.84393300e-03,  9.67191079e-04,\n",
       "         3.34065801e-04,  8.52980520e-04, -1.47393397e-04],\n",
       "       [ 1.56818840e+00,  1.88905577e+00, -2.53950516e-01,\n",
       "        -4.05890282e-02,  9.05041773e-02,  2.39678036e-02,\n",
       "         1.06480813e-02,  3.10842279e-02,  4.68415227e-02,\n",
       "         6.44417574e-02, -1.02294776e-01,  2.21547279e-02,\n",
       "        -3.69318550e-03,  1.88388057e-03,  1.13196884e-03,\n",
       "         1.03241327e-02,  1.61401460e-02, -2.30179593e-03,\n",
       "         1.70550894e-04,  2.83615548e-03, -5.31128352e-03,\n",
       "         1.09556680e-02, -2.60421405e-03,  1.33405416e-03,\n",
       "        -1.39081055e-03, -7.96492361e-04,  8.66910164e-04,\n",
       "        -2.80709000e-04, -3.16660619e-04, -1.54393122e-04],\n",
       "       [ 1.61091643e+00,  1.84041492e+00, -2.64396687e-01,\n",
       "        -2.32499239e-02,  1.10987834e-01,  2.80422307e-02,\n",
       "        -2.85433478e-02,  5.96896732e-02, -7.56403367e-03,\n",
       "         3.06143853e-02, -1.29275316e-01, -4.32452053e-02,\n",
       "        -2.79555477e-03, -3.43845474e-03,  4.04833270e-04,\n",
       "         1.50949905e-02, -6.08864876e-03, -1.63198592e-02,\n",
       "         1.18341699e-02,  2.07746167e-02, -2.40539581e-02,\n",
       "         3.35633458e-02, -1.85733973e-02, -2.02772771e-03,\n",
       "        -4.87328081e-04, -2.90790964e-03,  4.41855468e-04,\n",
       "        -2.99550061e-04, -4.90993785e-04, -1.86458749e-04],\n",
       "       [ 1.64093172e+00,  1.87978243e+00, -2.44295596e-01,\n",
       "        -3.09397187e-02,  7.53343726e-02, -4.90441874e-02,\n",
       "         3.97194045e-02, -2.98741552e-02,  1.16615410e-01,\n",
       "         1.85988896e-01, -3.62750664e-02,  2.42735880e-02,\n",
       "         3.23266034e-02, -1.14146283e-02, -1.97894990e-02,\n",
       "         2.55762205e-02,  3.14716527e-02,  2.30965991e-02,\n",
       "        -1.13609117e-02, -7.05731971e-03,  2.59356846e-02,\n",
       "        -5.58192740e-02,  3.93735398e-02, -7.27751923e-03,\n",
       "        -3.84432652e-03, -9.74711631e-04, -5.31583261e-04,\n",
       "        -5.85418699e-04, -4.12743388e-04, -9.05302168e-05],\n",
       "       [ 1.57507925e+00,  1.87160802e+00, -2.60834812e-01,\n",
       "        -4.28184763e-02,  1.00779537e-01,  2.54320242e-02,\n",
       "        -4.06104952e-03,  4.98661330e-02,  3.11115581e-02,\n",
       "         2.95614800e-02, -1.31178014e-01,  2.92767066e-03,\n",
       "        -3.29580803e-03,  6.40519992e-03,  5.21282720e-03,\n",
       "         1.17127603e-02,  1.12289388e-02, -1.06908235e-02,\n",
       "         2.59081577e-03,  7.83406156e-03, -1.72366087e-02,\n",
       "         3.27865713e-02, -1.62234167e-02,  2.49344163e-03,\n",
       "        -3.97972832e-04, -7.16400925e-04,  1.18471122e-03,\n",
       "        -1.74822200e-04, -2.82588114e-04, -1.75052695e-04]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = dim_size)\n",
    "x_reduced = svd.fit_transform(x_train)\n",
    "x_reduced[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 40)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_reduced = svd.transform(x_test)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(arr,y, num_seq):\n",
    "    \n",
    "    new_arr = np.zeros((arr.shape[0],num_seq,arr.shape[1]))\n",
    "    new_y = np.zeros(arr.shape[0])\n",
    "    for i in range(arr.shape[0]-num_seq):\n",
    "        for j in range(num_seq):\n",
    "            new_arr[i,j,:] = arr[i+j,:]\n",
    "        new_y[i] = y[i+num_seq-1]\n",
    "    x_resampled ,y_resampled = SMOTE().fit_sample(np.reshape(new_arr,[-1,num_seq*arr.shape[1]]), new_y)\n",
    "    x_resampled = np.reshape(x_resampled,[-1,num_seq,arr.shape[1]])\n",
    "    return x_resampled,y_resampled\n",
    "    #return new_arr,new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resampled = []\n",
    "y_resampled = []\n",
    "for i in range(y_train.size//1000):\n",
    "    x_resampledi, y_resampledi = resample(x_reduced[i*1000:1000*(i+1)], y_train[i*1000:1000*(i+1)],num_seq)\n",
    "    x_resampled.append(x_resampledi)\n",
    "    y_resampled.append(y_resampledi)\n",
    "#x_resampledj,y_resampledj = resample(x_reduced[75000:], y_train[75000:],num_seq)\n",
    "#x_resampled.append(x_resampledj)\n",
    "#y_resampled.append(y_resampledj)\n",
    "x_resampled = np.array(np.concatenate(x_resampled, axis = 0))    \n",
    "y_resampled = np.array(np.concatenate(y_resampled, axis = 0))\n",
    "from random import shuffle\n",
    "ind_list = [i for i in range(x_resampled.shape[0])]\n",
    "shuffle(ind_list)\n",
    "x_train_ = x_resampled[ind_list]\n",
    "y_train_ = y_resampled[ind_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid = x_train_[-40000:]\n",
    "y_valid = y_train_[-40000:]\n",
    "train_x = x_train_[:-40000]\n",
    "train_y = y_train_[:-40000]\n",
    "x_train.shape\n",
    "np.mean(train_y)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batches(arr,y, batch_size):\n",
    "    n_batches = arr.shape[0]//batch_size\n",
    "    for i in range(0,n_batches*batch_size,batch_size):\n",
    "        x = arr[i:i+batch_size,:,:]\n",
    "        y_ = y[i:i+batch_size]\n",
    "        yield x,y_\n",
    "x,y = next(get_batches(train_x, train_y, batch_size))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 1.01293012e+00,  4.35494664e-01,  2.09369311e+00, ...,\n",
       "           3.49895539e-04,  1.06489316e-03, -5.03614168e-05]],\n",
       " \n",
       "        [[ 9.62122067e-01,  4.40164641e-01,  2.08573104e+00, ...,\n",
       "           5.94780357e-04,  1.19269372e-03, -9.71355139e-05]],\n",
       " \n",
       "        [[ 1.53995130e+00, -4.51072468e-01,  2.55773964e-02, ...,\n",
       "           1.11691458e-03,  1.24406357e-03, -1.02300494e-04]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 2.13825193e+00, -5.76708062e-01, -4.03953467e-03, ...,\n",
       "           3.87577404e-04,  2.64297080e-04, -1.27516369e-04]],\n",
       " \n",
       "        [[ 1.86204993e+00,  1.29133525e+00, -3.34787269e-01, ...,\n",
       "           2.87780141e-03,  3.02120155e-03,  1.59356700e-03]],\n",
       " \n",
       "        [[ 1.09115856e+00,  4.25425984e-01,  2.09388678e+00, ...,\n",
       "           4.67406576e-04,  2.06359524e-03, -1.52924506e-04]]]),\n",
       " array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_test_batches(arr,y, batch_size, num_seq=num_seq):\n",
    "    new_arr = np.zeros((arr.shape[0],num_seq,arr.shape[1]))\n",
    "    new_y = np.zeros(arr.shape[0])\n",
    "    for i in range(arr.shape[0]-num_seq):\n",
    "        for j in range(num_seq):\n",
    "            new_arr[i,j,:] = arr[i+j,:]\n",
    "        new_y[i] = y[i+num_seq-1]\n",
    "    n_batches = arr.shape[0]//batch_size\n",
    "    for i in range(0,n_batches*batch_size,batch_size):\n",
    "        x = new_arr[i:i+batch_size,:,:]\n",
    "        y = new_y[i:i+batch_size]\n",
    "        yield x,y\n",
    "next(get_test_batches(x_test_reduced, y_test,100))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_inputs = 100 ):\n",
    "    inputs = tf.placeholder(tf.float32, [None,num_inputs,dim_size], name = 'inputs')\n",
    "    outputs = tf.placeholder(tf.int32, [None,],name = 'targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, outputs, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, initial_state   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size, relu_size = 100):\n",
    "    out = tf.contrib.layers.fully_connected(lstm_output, relu_size, activation_fn = tf.nn.relu)\n",
    "    out = tf.layers.batch_normalization(out)\n",
    "    logits = tf.contrib.layers.fully_connected(out,out_size, activation_fn=None)\n",
    "    #logits = tf.reshape(logits, (-1,10,3, out_size))\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets,vocab_size):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "      # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    #logits = tf.reshape(logits,[-1,vocab_size])\n",
    "    y_one_hot = tf.one_hot(targets, vocab_size)\n",
    "    #y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    #y_reshaped = tf.reduce_sum(y_one_hot, axis = 1)\n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss, y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, vocab_size, relu_size, batch_size=64,\n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_inputs = num_seq )\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "       \n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, self.inputs, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.logits = build_output(outputs, lstm_size, vocab_size, relu_size = relu_size)[:,-1]\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss, self.y_reshaped = build_loss(self.logits, self.targets, vocab_size)\n",
    "        self.pred = tf.argmax(self.logits,1)\n",
    "        #_,self.pred = tf.nn.top_k(self.logits,3)\n",
    "        correct_pred = tf.equal(self.pred, tf.argmax(self.y_reshaped,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Training loss : 0.174829\n",
      "Validation loss : 0.057415\n",
      "epoch 1: Training loss : 0.051725\n",
      "Validation loss : 0.039678\n",
      "epoch 2: Training loss : 0.040569\n",
      "Validation loss : 0.030937\n",
      "epoch 3: Training loss : 0.033897\n",
      "Validation loss : 0.024985\n",
      "epoch 4: Training loss : 0.027858\n",
      "Validation loss : 0.020762\n",
      "epoch 5: Training loss : 0.024964\n",
      "Validation loss : 0.017981\n",
      "epoch 6: Training loss : 0.021313\n",
      "Validation loss : 0.015026\n",
      "epoch 7: Training loss : 0.018916\n",
      "Validation loss : 0.012329\n",
      "epoch 8: Training loss : 0.016096\n",
      "Validation loss : 0.010338\n",
      "epoch 9: Training loss : 0.014115\n",
      "Validation loss : 0.009010\n",
      "epoch 10: Training loss : 0.012723\n",
      "Validation loss : 0.007875\n",
      "epoch 11: Training loss : 0.011675\n",
      "Validation loss : 0.006913\n",
      "epoch 12: Training loss : 0.010406\n",
      "Validation loss : 0.006475\n",
      "epoch 13: Training loss : 0.009526\n",
      "Validation loss : 0.005756\n",
      "epoch 14: Training loss : 0.008655\n",
      "Validation loss : 0.005251\n",
      "epoch 15: Training loss : 0.007904\n",
      "Validation loss : 0.004933\n",
      "epoch 16: Training loss : 0.007545\n",
      "Validation loss : 0.004596\n",
      "epoch 17: Training loss : 0.007270\n",
      "Validation loss : 0.004680\n",
      "epoch 18: Training loss : 0.007012\n",
      "Validation loss : 0.004322\n",
      "epoch 19: Training loss : 0.006366\n",
      "Validation loss : 0.004231\n",
      "epoch 20: Training loss : 0.006328\n",
      "Validation loss : 0.004061\n",
      "epoch 21: Training loss : 0.005604\n",
      "Validation loss : 0.004054\n",
      "epoch 22: Training loss : 0.005575\n",
      "Validation loss : 0.003979\n",
      "epoch 23: Training loss : 0.005494\n",
      "Validation loss : 0.003915\n",
      "epoch 24: Training loss : 0.005046\n",
      "Validation loss : 0.003840\n",
      "epoch 25: Training loss : 0.004675\n",
      "Validation loss : 0.003822\n",
      "epoch 26: Training loss : 0.004898\n",
      "Validation loss : 0.003700\n",
      "epoch 27: Training loss : 0.004432\n",
      "Validation loss : 0.004123\n",
      "epoch 28: Training loss : 0.004460\n",
      "Validation loss : 0.003855\n",
      "epoch 29: Training loss : 0.004659\n",
      "Validation loss : 0.003818\n",
      "epoch 30: Training loss : 0.004490\n",
      "Validation loss : 0.003984\n",
      "epoch 31: Training loss : 0.004610\n",
      "Validation loss : 0.003870\n",
      "epoch 32: Training loss : 0.004091\n",
      "Validation loss : 0.003908\n",
      "epoch 33: Training loss : 0.004075\n",
      "Validation loss : 0.003836\n",
      "epoch 34: Training loss : 0.003891\n",
      "Validation loss : 0.003775\n",
      "epoch 35: Training loss : 0.003871\n",
      "Validation loss : 0.003664\n",
      "epoch 36: Training loss : 0.003968\n",
      "Validation loss : 0.003728\n",
      "epoch 37: Training loss : 0.003738\n",
      "Validation loss : 0.003810\n",
      "epoch 38: Training loss : 0.003861\n",
      "Validation loss : 0.003719\n",
      "epoch 39: Training loss : 0.003903\n",
      "Validation loss : 0.003553\n",
      "epoch 40: Training loss : 0.003426\n",
      "Validation loss : 0.003662\n",
      "epoch 41: Training loss : 0.003507\n",
      "Validation loss : 0.003667\n",
      "epoch 42: Training loss : 0.003702\n",
      "Validation loss : 0.003841\n",
      "epoch 43: Training loss : 0.003413\n",
      "Validation loss : 0.003939\n",
      "epoch 44: Training loss : 0.003375\n",
      "Validation loss : 0.003709\n",
      "epoch 45: Training loss : 0.003540\n",
      "Validation loss : 0.004091\n",
      "epoch 46: Training loss : 0.003654\n",
      "Validation loss : 0.003656\n",
      "epoch 47: Training loss : 0.003362\n",
      "Validation loss : 0.003678\n",
      "epoch 48: Training loss : 0.003241\n",
      "Validation loss : 0.003927\n",
      "epoch 49: Training loss : 0.003008\n",
      "Validation loss : 0.003970\n",
      "epoch 50: Training loss : 0.003268\n",
      "Validation loss : 0.003721\n",
      "epoch 51: Training loss : 0.003151\n",
      "Validation loss : 0.003759\n",
      "epoch 52: Training loss : 0.003062\n",
      "Validation loss : 0.003656\n",
      "epoch 53: Training loss : 0.002895\n",
      "Validation loss : 0.003511\n",
      "epoch 54: Training loss : 0.002878\n",
      "Validation loss : 0.003653\n",
      "epoch 55: Training loss : 0.002629\n",
      "Validation loss : 0.003840\n",
      "epoch 56: Training loss : 0.003300\n",
      "Validation loss : 0.003755\n",
      "epoch 57: Training loss : 0.002964\n",
      "Validation loss : 0.003935\n",
      "epoch 58: Training loss : 0.002917\n",
      "Validation loss : 0.003574\n",
      "epoch 59: Training loss : 0.002829\n",
      "Validation loss : 0.003731\n",
      "epoch 60: Training loss : 0.002806\n",
      "Validation loss : 0.003765\n",
      "epoch 61: Training loss : 0.002540\n",
      "Validation loss : 0.003778\n",
      "epoch 62: Training loss : 0.002629\n",
      "Validation loss : 0.003933\n",
      "epoch 63: Training loss : 0.002793\n",
      "Validation loss : 0.003931\n",
      "epoch 64: Training loss : 0.002669\n",
      "Validation loss : 0.003888\n",
      "epoch 65: Training loss : 0.002589\n",
      "Validation loss : 0.004138\n",
      "epoch 66: Training loss : 0.002563\n",
      "Validation loss : 0.003801\n",
      "epoch 67: Training loss : 0.002451\n",
      "Validation loss : 0.003986\n",
      "epoch 68: Training loss : 0.002637\n",
      "Validation loss : 0.003804\n",
      "epoch 69: Training loss : 0.002646\n",
      "Validation loss : 0.003992\n",
      "epoch 70: Training loss : 0.002709\n",
      "Validation loss : 0.003760\n",
      "epoch 71: Training loss : 0.002547\n",
      "Validation loss : 0.003738\n",
      "epoch 72: Training loss : 0.002382\n",
      "Validation loss : 0.003968\n",
      "epoch 73: Training loss : 0.002461\n",
      "Validation loss : 0.003705\n",
      "epoch 74: Training loss : 0.002505\n",
      "Validation loss : 0.003722\n",
      "epoch 75: Training loss : 0.002390\n",
      "Validation loss : 0.004009\n",
      "epoch 76: Training loss : 0.002364\n",
      "Validation loss : 0.003637\n",
      "epoch 77: Training loss : 0.002429\n",
      "Validation loss : 0.003828\n",
      "epoch 78: Training loss : 0.002360\n",
      "Validation loss : 0.003749\n",
      "epoch 79: Training loss : 0.002396\n",
      "Validation loss : 0.003985\n",
      "epoch 80: Training loss : 0.002205\n",
      "Validation loss : 0.003697\n",
      "epoch 81: Training loss : 0.002313\n",
      "Validation loss : 0.004050\n",
      "epoch 82: Training loss : 0.002520\n",
      "Validation loss : 0.004048\n",
      "epoch 83: Training loss : 0.002282\n",
      "Validation loss : 0.003774\n",
      "epoch 84: Training loss : 0.002347\n",
      "Validation loss : 0.003812\n",
      "epoch 85: Training loss : 0.002180\n",
      "Validation loss : 0.004092\n",
      "epoch 86: Training loss : 0.002248\n",
      "Validation loss : 0.003857\n",
      "epoch 87: Training loss : 0.002160\n",
      "Validation loss : 0.004371\n",
      "epoch 88: Training loss : 0.002121\n",
      "Validation loss : 0.003808\n",
      "epoch 89: Training loss : 0.002234\n",
      "Validation loss : 0.003981\n",
      "epoch 90: Training loss : 0.002180\n",
      "Validation loss : 0.003919\n",
      "epoch 91: Training loss : 0.002450\n",
      "Validation loss : 0.003698\n",
      "epoch 92: Training loss : 0.002363\n",
      "Validation loss : 0.003519\n",
      "epoch 93: Training loss : 0.001947\n",
      "Validation loss : 0.004055\n",
      "epoch 94: Training loss : 0.002218\n",
      "Validation loss : 0.003717\n",
      "epoch 95: Training loss : 0.001796\n",
      "Validation loss : 0.003774\n",
      "epoch 96: Training loss : 0.002136\n",
      "Validation loss : 0.004068\n",
      "epoch 97: Training loss : 0.002129\n",
      "Validation loss : 0.004190\n",
      "epoch 98: Training loss : 0.001899\n",
      "Validation loss : 0.004202\n",
      "epoch 99: Training loss : 0.002061\n",
      "Validation loss : 0.003925\n",
      "epoch 100: Training loss : 0.002141\n",
      "Validation loss : 0.004195\n",
      "epoch 101: Training loss : 0.001926\n",
      "Validation loss : 0.004231\n",
      "epoch 102: Training loss : 0.002042\n",
      "Validation loss : 0.003959\n",
      "epoch 103: Training loss : 0.002160\n",
      "Validation loss : 0.004193\n",
      "epoch 104: Training loss : 0.002304\n",
      "Validation loss : 0.003958\n",
      "epoch 105: Training loss : 0.002013\n",
      "Validation loss : 0.004197\n",
      "epoch 106: Training loss : 0.002338\n",
      "Validation loss : 0.003733\n",
      "epoch 107: Training loss : 0.001942\n",
      "Validation loss : 0.003748\n",
      "epoch 108: Training loss : 0.001982\n",
      "Validation loss : 0.003802\n",
      "epoch 109: Training loss : 0.002106\n",
      "Validation loss : 0.003900\n",
      "epoch 110: Training loss : 0.001915\n",
      "Validation loss : 0.003956\n",
      "epoch 111: Training loss : 0.002229\n",
      "Validation loss : 0.003649\n",
      "epoch 112: Training loss : 0.002017\n",
      "Validation loss : 0.004159\n",
      "epoch 113: Training loss : 0.001812\n",
      "Validation loss : 0.004043\n",
      "epoch 114: Training loss : 0.001943\n",
      "Validation loss : 0.003892\n",
      "epoch 115: Training loss : 0.001968\n",
      "Validation loss : 0.004216\n",
      "epoch 116: Training loss : 0.001701\n",
      "Validation loss : 0.004139\n",
      "epoch 117: Training loss : 0.002010\n",
      "Validation loss : 0.004311\n",
      "epoch 118: Training loss : 0.001988\n",
      "Validation loss : 0.004136\n",
      "epoch 119: Training loss : 0.001758\n",
      "Validation loss : 0.004438\n",
      "epoch 120: Training loss : 0.001807\n",
      "Validation loss : 0.003996\n",
      "epoch 121: Training loss : 0.001824\n",
      "Validation loss : 0.004212\n",
      "epoch 122: Training loss : 0.001958\n",
      "Validation loss : 0.004484\n",
      "epoch 123: Training loss : 0.001906\n",
      "Validation loss : 0.004409\n",
      "epoch 124: Training loss : 0.001956\n",
      "Validation loss : 0.004306\n",
      "epoch 125: Training loss : 0.001653\n",
      "Validation loss : 0.004309\n",
      "epoch 126: Training loss : 0.001728\n",
      "Validation loss : 0.004413\n",
      "epoch 127: Training loss : 0.001759\n",
      "Validation loss : 0.004313\n",
      "epoch 128: Training loss : 0.001799\n",
      "Validation loss : 0.004378\n",
      "epoch 129: Training loss : 0.001982\n",
      "Validation loss : 0.004315\n",
      "epoch 130: Training loss : 0.001902\n",
      "Validation loss : 0.004311\n",
      "epoch 131: Training loss : 0.001630\n",
      "Validation loss : 0.004249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 132: Training loss : 0.001600\n",
      "Validation loss : 0.004045\n",
      "epoch 133: Training loss : 0.001761\n",
      "Validation loss : 0.004635\n",
      "epoch 134: Training loss : 0.001845\n",
      "Validation loss : 0.004451\n",
      "epoch 135: Training loss : 0.001807\n",
      "Validation loss : 0.004291\n",
      "epoch 136: Training loss : 0.001686\n",
      "Validation loss : 0.003953\n",
      "epoch 137: Training loss : 0.001634\n",
      "Validation loss : 0.004138\n",
      "epoch 138: Training loss : 0.001779\n",
      "Validation loss : 0.004283\n",
      "epoch 139: Training loss : 0.001714\n",
      "Validation loss : 0.004339\n",
      "epoch 140: Training loss : 0.001661\n",
      "Validation loss : 0.004900\n",
      "epoch 141: Training loss : 0.001881\n",
      "Validation loss : 0.004330\n",
      "epoch 142: Training loss : 0.001560\n",
      "Validation loss : 0.004280\n",
      "epoch 143: Training loss : 0.001587\n",
      "Validation loss : 0.004353\n",
      "epoch 144: Training loss : 0.001524\n",
      "Validation loss : 0.004520\n",
      "epoch 145: Training loss : 0.001670\n",
      "Validation loss : 0.004615\n",
      "epoch 146: Training loss : 0.001774\n",
      "Validation loss : 0.004217\n",
      "epoch 147: Training loss : 0.001457\n",
      "Validation loss : 0.004313\n",
      "epoch 148: Training loss : 0.001860\n",
      "Validation loss : 0.004308\n",
      "epoch 149: Training loss : 0.001323\n",
      "Validation loss : 0.004358\n",
      "epoch 150: Training loss : 0.001691\n",
      "Validation loss : 0.004644\n",
      "epoch 151: Training loss : 0.001582\n",
      "Validation loss : 0.004861\n",
      "epoch 152: Training loss : 0.001697\n",
      "Validation loss : 0.004800\n",
      "epoch 153: Training loss : 0.001873\n",
      "Validation loss : 0.004823\n",
      "epoch 154: Training loss : 0.001739\n",
      "Validation loss : 0.004476\n",
      "epoch 155: Training loss : 0.001449\n",
      "Validation loss : 0.004296\n",
      "epoch 156: Training loss : 0.001494\n",
      "Validation loss : 0.004419\n",
      "epoch 157: Training loss : 0.001353\n",
      "Validation loss : 0.004333\n",
      "epoch 158: Training loss : 0.001582\n",
      "Validation loss : 0.004352\n",
      "epoch 159: Training loss : 0.001522\n",
      "Validation loss : 0.004692\n",
      "epoch 160: Training loss : 0.001549\n",
      "Validation loss : 0.004380\n",
      "epoch 161: Training loss : 0.001427\n",
      "Validation loss : 0.004378\n",
      "epoch 162: Training loss : 0.001514\n",
      "Validation loss : 0.004222\n",
      "epoch 163: Training loss : 0.001472\n",
      "Validation loss : 0.004503\n",
      "epoch 164: Training loss : 0.001808\n",
      "Validation loss : 0.004162\n",
      "epoch 165: Training loss : 0.001408\n",
      "Validation loss : 0.004297\n",
      "epoch 166: Training loss : 0.001678\n",
      "Validation loss : 0.003866\n",
      "epoch 167: Training loss : 0.001595\n",
      "Validation loss : 0.003994\n",
      "epoch 168: Training loss : 0.001406\n",
      "Validation loss : 0.004271\n",
      "epoch 169: Training loss : 0.001378\n",
      "Validation loss : 0.003988\n",
      "epoch 170: Training loss : 0.001388\n",
      "Validation loss : 0.004303\n",
      "epoch 171: Training loss : 0.001546\n",
      "Validation loss : 0.004142\n",
      "epoch 172: Training loss : 0.001454\n",
      "Validation loss : 0.004179\n",
      "epoch 173: Training loss : 0.001416\n",
      "Validation loss : 0.004001\n",
      "epoch 174: Training loss : 0.001567\n",
      "Validation loss : 0.004595\n",
      "epoch 175: Training loss : 0.001469\n",
      "Validation loss : 0.004373\n",
      "epoch 176: Training loss : 0.001513\n",
      "Validation loss : 0.004253\n",
      "epoch 177: Training loss : 0.001211\n",
      "Validation loss : 0.004264\n",
      "epoch 178: Training loss : 0.001455\n",
      "Validation loss : 0.004820\n",
      "epoch 179: Training loss : 0.001469\n",
      "Validation loss : 0.004253\n",
      "epoch 180: Training loss : 0.001336\n",
      "Validation loss : 0.004411\n",
      "epoch 181: Training loss : 0.001351\n",
      "Validation loss : 0.004113\n",
      "epoch 182: Training loss : 0.001311\n",
      "Validation loss : 0.004308\n",
      "epoch 183: Training loss : 0.001485\n",
      "Validation loss : 0.004087\n",
      "epoch 184: Training loss : 0.001424\n",
      "Validation loss : 0.004334\n",
      "epoch 185: Training loss : 0.001289\n",
      "Validation loss : 0.004436\n",
      "epoch 186: Training loss : 0.001368\n",
      "Validation loss : 0.004257\n",
      "epoch 187: Training loss : 0.001296\n",
      "Validation loss : 0.004089\n",
      "epoch 188: Training loss : 0.001383\n",
      "Validation loss : 0.004437\n",
      "epoch 189: Training loss : 0.001266\n",
      "Validation loss : 0.004159\n",
      "epoch 190: Training loss : 0.001193\n",
      "Validation loss : 0.004320\n",
      "epoch 191: Training loss : 0.001459\n",
      "Validation loss : 0.004385\n",
      "epoch 192: Training loss : 0.001276\n",
      "Validation loss : 0.004451\n",
      "epoch 193: Training loss : 0.001328\n",
      "Validation loss : 0.004376\n",
      "epoch 194: Training loss : 0.001503\n",
      "Validation loss : 0.004531\n",
      "epoch 195: Training loss : 0.001258\n",
      "Validation loss : 0.004739\n",
      "epoch 196: Training loss : 0.001235\n",
      "Validation loss : 0.004406\n",
      "epoch 197: Training loss : 0.001306\n",
      "Validation loss : 0.004617\n",
      "epoch 198: Training loss : 0.001299\n",
      "Validation loss : 0.004284\n",
      "epoch 199: Training loss : 0.001326\n",
      "Validation loss : 0.004207\n",
      "Training accuracy : 99.98\n",
      "validation accuracy : 99.92\n",
      "Test accuracy : 96.18\n"
     ]
    }
   ],
   "source": [
    "save_every_n = 200\n",
    "\n",
    "model = RNN(2,relu_size, batch_size=batch_size,\n",
    "                lstm_size=rnn_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        train_loss = []\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(train_x, train_y,batch_size):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            train_loss.append(batch_loss)\n",
    "            #if(counter%show_every_n_batches ==0):\n",
    "                #print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  #'Training Step: {}... '.format(counter),\n",
    "                    #'Training loss: {:.4f}... '.format(batch_loss))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, num_layers))\n",
    "        val_loss = []\n",
    "        val_state = sess.run(model.initial_state)\n",
    "        for x, y in get_batches(x_valid, y_valid, batch_size):\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "            batch_loss, val_state = sess.run([model.loss, \n",
    "                                                 model.final_state], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "                                                 \n",
    "            val_loss.append(batch_loss)\n",
    "            \n",
    "        print(\"epoch {}: Training loss : {:.6f}\".format(e, np.mean(train_loss)))\n",
    "        print(\"Validation loss : {:.6f}\".format(np.mean(val_loss)))\n",
    "        \n",
    "    val_acc = []\n",
    "    val_pred =[]\n",
    "    val_y = []\n",
    "    val_logits = []\n",
    "    val_state = sess.run(model.initial_state)\n",
    "    for x, y in get_batches(x_valid, y_valid,batch_size):\n",
    "        feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "        val_state,batch_pred,batch_acc,batch_logits = sess.run([model.final_state,model.pred,model.accuracy, model.logits], feed_dict = feed)\n",
    "        val_acc.append(batch_acc)\n",
    "        val_y.append(y)\n",
    "        val_pred.append(batch_pred)\n",
    "        val_logits.append(batch_logits)\n",
    "    train_pred =[]\n",
    "    train_label = []\n",
    "    train_acc = []\n",
    "    train_logits = []\n",
    "    train_state = sess.run(model.initial_state)\n",
    "    for x, y in get_batches(train_x, train_y,batch_size):\n",
    "        feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: train_state}\n",
    "        train_state,batch_pred,batch_acc, batch_logits = sess.run([model.final_state,model.pred,model.accuracy, model.logits], feed_dict = feed)\n",
    "        train_acc.append(batch_acc)\n",
    "        train_label.append(y)\n",
    "        train_pred.append(batch_pred)\n",
    "        train_logits.append(batch_logits)\n",
    "    print(\"Training accuracy : {:.2f}\".format(100*np.mean(train_acc)))    \n",
    "    print(\"validation accuracy : {:.2f}\".format(100*np.mean(val_acc)))\n",
    "    test_acc = []\n",
    "    test_pred =[]\n",
    "    test_y = []\n",
    "    test_logits = []\n",
    "    test_state = sess.run(model.initial_state)\n",
    "    for x, y in get_test_batches(x_test_reduced, y_test,batch_size,num_seq):\n",
    "        feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "        test_state,batch_pred,batch_acc, batch_logits = sess.run([model.final_state,model.pred,model.accuracy,model.logits], feed_dict = feed)\n",
    "        test_acc.append(batch_acc)\n",
    "        test_y.append(y)\n",
    "        test_pred.append(batch_pred)\n",
    "        test_logits.append(batch_logits)\n",
    "    print(\"Test accuracy : {:.2f}\".format(100*np.mean(test_acc))) \n",
    "    #val_pred = np.vstack(val_pred)\n",
    "    #val_y = np.vstack(val_y)\n",
    "    #print(mapk(val_y,val_pred))\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20112 20113.0\n",
      "6272\n"
     ]
    }
   ],
   "source": [
    "y_ =[]\n",
    "print(np.sum([np.sum(pred)for pred in val_pred]), np.sum([np.sum(pred)for pred in val_y]))\n",
    "print(np.sum([np.sum(pred) for pred in test_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5562, 710, 145, 15983)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn,fp,fn,tp = confusion_matrix(np.concatenate(test_y),np.concatenate(test_pred)).ravel()\n",
    "(tp,fp,fn,tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8867984693877551,\n",
       " 0.042532798178877375,\n",
       " 0.9745926055721045,\n",
       " 0.9286250939143501,\n",
       " 0.96183044)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[75000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118940, 1, 30)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(np.reshape(train_x,[-1,30]),train_y)\n",
    "val_pred = clf.predict(np.reshape(x_valid,[-1,30]))\n",
    "test_pred = clf.predict(x_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18429, 8805, 1684, 18429)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn,fp,fn,tp = confusion_matrix(y_valid,val_pred).ravel()\n",
    "(tp,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6766909010795329,\n",
       " 0.44275154623623475,\n",
       " 0.9162730572266693,\n",
       " 0.778465372674087,\n",
       " 0.737775)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999932505399568,\n",
       " 6.720317198971792e-05,\n",
       " 0.9996794817726344,\n",
       " 0.9998059775777565,\n",
       " 0.9998063973063973)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(np.concatenate(train_label,0),np.concatenate(train_pred,0)).ravel()\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_logits\n",
    "clf.fit(np.concatenate(train_logits,0),train_y[:118800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf.predict(np.concatenate(val_logits,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19606, 2, 507, 19606)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn,fp,fn,tp = confusion_matrix(y_valid,val_pred).ravel()\n",
    "(tp,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9998980008159934,\n",
       " 0.00010056821038869613,\n",
       " 0.9747924228111172,\n",
       " 0.9871856196973894,\n",
       " 0.987275)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9643122676579926,\n",
       " 0.011501827113161205,\n",
       " 0.9090590502891186,\n",
       " 0.9358708397221971,\n",
       " 0.9682589285714286)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_ = clf.predict(np.concatenate(test_logits,0))\n",
    "tn,fp,fn,tp = confusion_matrix(np.concatenate(test_y,0),test_pred_).ravel()\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "DR = tp/(tp+fp)\n",
    "FAR = fp/(tn+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*(recall*DR)/(recall+DR)\n",
    "(DR,FAR,recall,f1,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
